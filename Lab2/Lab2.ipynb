{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Task 1",
   "id": "bb36f4b803491e93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T19:12:05.821039Z",
     "start_time": "2025-10-14T19:12:05.818035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "# Corpus declaration\n",
    "corpus = [\n",
    "    \"there is a big house\",\n",
    "    \"i buy a house\",\n",
    "    \"they buy the new house\",\n",
    "]"
   ],
   "id": "94c78931772cabc5",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T19:12:06.402885Z",
     "start_time": "2025-10-14T19:12:05.853920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Declare the BPE tokenizer to divide the corpus into words\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Word frequency\n",
    "word_freqs = defaultdict(int)\n",
    "\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "print(\"Word frequencies:\")\n",
    "print(word_freqs)\n",
    "print(\"================\")"
   ],
   "id": "a390fb60c4b5f4b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word frequencies:\n",
      "defaultdict(<class 'int'>, {'there': 1, 'Ġis': 1, 'Ġa': 2, 'Ġbig': 1, 'Ġhouse': 3, 'i': 1, 'Ġbuy': 2, 'they': 1, 'Ġthe': 1, 'Ġnew': 1})\n",
      "================\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T19:12:06.430821Z",
     "start_time": "2025-10-14T19:12:06.427365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Compute base vocabulary\n",
    "# First, extract the present letters (the alphabet)\n",
    "\n",
    "alphabet = []\n",
    "\n",
    "for word in word_freqs.keys():\n",
    "    for letter in word:\n",
    "        if letter not in alphabet:\n",
    "            alphabet.append(letter)\n",
    "alphabet.sort()\n",
    "\n",
    "print(\"The alphabet: \")\n",
    "print(alphabet)\n",
    "print(\"================\")\n",
    "\n",
    "vocab = [\"<|endoftext|>\"] + alphabet.copy()"
   ],
   "id": "3a0b217d0ef81c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alphabet: \n",
      "['a', 'b', 'e', 'g', 'h', 'i', 'n', 'o', 'r', 's', 't', 'u', 'w', 'y', 'Ġ']\n",
      "================\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T19:12:06.442359Z",
     "start_time": "2025-10-14T19:12:06.439300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Split words into individual characters\n",
    "splits = {word: [c for c in word] for word in word_freqs.keys()}"
   ],
   "id": "ba7f6ab77bf09333",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T19:12:06.453505Z",
     "start_time": "2025-10-14T19:12:06.450016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# The BPE algorithm is based on most frequent pair and merging\n",
    "# Helper function to compute the most frequent pair\n",
    "\n",
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs"
   ],
   "id": "c38dcfa5383ab18f",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T19:12:06.464730Z",
     "start_time": "2025-10-14T19:12:06.461280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute initial pairs\n",
    "pair_freqs = compute_pair_freqs(splits)\n",
    "\n",
    "for i, key in enumerate(pair_freqs.keys()):\n",
    "    print(f\"{key}: {pair_freqs[key]}\")\n",
    "    if i >= 5:\n",
    "        break"
   ],
   "id": "31ea6299de7d7ae8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('t', 'h'): 3\n",
      "('h', 'e'): 3\n",
      "('e', 'r'): 1\n",
      "('r', 'e'): 1\n",
      "('Ġ', 'i'): 1\n",
      "('i', 's'): 1\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T19:12:06.476725Z",
     "start_time": "2025-10-14T19:12:06.472712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get most frequent pair\n",
    "best_pair = \"\"\n",
    "max_freq = None\n",
    "\n",
    "for pair, freq in pair_freqs.items():\n",
    "    if max_freq is None or max_freq < freq:\n",
    "        best_pair = pair\n",
    "        max_freq = freq\n",
    "\n",
    "print(best_pair, max_freq)"
   ],
   "id": "6327ceda92786b99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('t', 'h') 3\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T19:12:06.487620Z",
     "start_time": "2025-10-14T19:12:06.484811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add the merge to the vocab\n",
    "merges = {(\"t\", \"h\"): \"th\"}\n",
    "vocab.append(\"th\")"
   ],
   "id": "dc564f3bdcd563a",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T19:12:06.498897Z",
     "start_time": "2025-10-14T19:12:06.495432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to merge the pair in the splits dictionary\n",
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits"
   ],
   "id": "e608daa51eeacb49",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T19:12:06.510117Z",
     "start_time": "2025-10-14T19:12:06.506741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "splits = merge_pair(\"t\", \"h\", splits)\n",
    "print(splits[\"they\"])"
   ],
   "id": "6d37011a6ffdd8df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['th', 'e', 'y']\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T19:12:06.522448Z",
     "start_time": "2025-10-14T19:12:06.519051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loop to grow the vocab size with the new merges\n",
    "vocab_size = 10\n",
    "\n",
    "while len(vocab) < vocab_size:\n",
    "    pair_freqs = compute_pair_freqs(splits)\n",
    "    best_pair = \"\"\n",
    "    max_freq = None\n",
    "    for pair, freq in pair_freqs.items():\n",
    "        if max_freq is None or max_freq < freq:\n",
    "            best_pair = pair\n",
    "            max_freq = freq\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])"
   ],
   "id": "59e74b751033b2dd",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T19:12:06.533790Z",
     "start_time": "2025-10-14T19:12:06.530751Z"
    }
   },
   "cell_type": "code",
   "source": "print(merges)",
   "id": "da83cecd5cee19ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('t', 'h'): 'th'}\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T19:12:06.546276Z",
     "start_time": "2025-10-14T19:12:06.542983Z"
    }
   },
   "cell_type": "code",
   "source": "print(vocab)",
   "id": "56dfa7b93382c0a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'a', 'b', 'e', 'g', 'h', 'i', 'n', 'o', 'r', 's', 't', 'u', 'w', 'y', 'Ġ', 'th']\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T19:12:06.560209Z",
     "start_time": "2025-10-14T19:12:06.556430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# To tokenize a new text, we pre-tokenize it, split it and aplpy the merge rules learned\n",
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "\n",
    "    return sum(splits, [])"
   ],
   "id": "fffea69a9c869e1c",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T19:12:06.573970Z",
     "start_time": "2025-10-14T19:12:06.570165Z"
    }
   },
   "cell_type": "code",
   "source": "tokenize(\"they buy a big house\")",
   "id": "d25198f11ea90adc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['th',\n",
       " 'e',\n",
       " 'y',\n",
       " 'Ġ',\n",
       " 'b',\n",
       " 'u',\n",
       " 'y',\n",
       " 'Ġ',\n",
       " 'a',\n",
       " 'Ġ',\n",
       " 'b',\n",
       " 'i',\n",
       " 'g',\n",
       " 'Ġ',\n",
       " 'h',\n",
       " 'o',\n",
       " 'u',\n",
       " 's',\n",
       " 'e']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Task 4\n",
   "id": "fd9d37880f88325"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T19:12:06.585993Z",
     "start_time": "2025-10-14T19:12:06.582134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import string\n",
    "from transformers import (\n",
    "    BertTokenizer, BertForMaskedLM,\n",
    "    AutoTokenizer, AutoModelWithLMHead,\n",
    "    XLNetTokenizer, XLNetLMHeadModel,\n",
    "    logging\n",
    ")\n",
    "logging.set_verbosity_error()"
   ],
   "id": "36c84a81ab942962",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T19:12:06.598879Z",
     "start_time": "2025-10-14T19:12:06.596276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "no_words_to_be_predicted = globals()\n",
    "select_model = globals()\n",
    "enter_input_text = globals()"
   ],
   "id": "db78769926f0633f",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T19:12:06.612200Z",
     "start_time": "2025-10-14T19:12:06.608876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_model_config(**kwargs):\n",
    "  for key, value in kwargs.items():\n",
    "    print(\"{0} = {1}\".format(key, value))\n",
    "\n",
    "  no_words_to_be_predicted = list(kwargs.values())[0] # integer values\n",
    "  select_model = list(kwargs.values())[1] # possible values = 'bert' or 'gpt' or 'xlnet'\n",
    "  enter_input_text = list(kwargs.values())[2] #only string\n",
    "\n",
    "  return no_words_to_be_predicted, select_model, enter_input_text"
   ],
   "id": "dcafaf312f9462c9",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T19:12:06.638013Z",
     "start_time": "2025-10-14T19:12:06.634280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_model(model_name):\n",
    "    if \"bert\" in model_name.lower():\n",
    "      bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "      bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased').eval()\n",
    "      return bert_tokenizer,bert_model\n",
    "    elif \"gpt\" in model_name.lower():\n",
    "      gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "      gpt_model = AutoModelWithLMHead.from_pretrained(\"gpt2\")\n",
    "      print(f\"Tokenizer: {gpt_tokenizer}, gpt_model: {gpt_model}\")\n",
    "      return gpt_tokenizer,gpt_model\n",
    "    else:\n",
    "      xlnet_tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "      xlnet_model = AutoModelWithLMHead.from_pretrained(\"xlnet-base-cased\")\n",
    "      return xlnet_tokenizer, xlnet_model"
   ],
   "id": "a57c88b856124a56",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T19:12:06.650827Z",
     "start_time": "2025-10-14T19:12:06.646748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# bert encode\n",
    "def encode_bert(tokenizer, text_sentence, add_special_tokens=True):\n",
    "  text_sentence = text_sentence.replace('<mask>', tokenizer.mask_token)\n",
    "  # if <mask> is the last token, append a \".\" so that models dont predict punctuation.\n",
    "  if tokenizer.mask_token == text_sentence.split()[-1]:\n",
    "    text_sentence += ' .'\n",
    "    input_ids = torch.tensor([tokenizer.encode(text_sentence, add_special_tokens=add_special_tokens)])\n",
    "    mask_idx = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]\n",
    "  return input_ids, mask_idx\n",
    "\n",
    "# bert decode\n",
    "def decode_bert(tokenizer, pred_idx, top_clean):\n",
    "  ignore_tokens = string.punctuation + '[PAD]'\n",
    "  tokens = []\n",
    "  for w in pred_idx:\n",
    "    token = ''.join(tokenizer.decode(w).split())\n",
    "    if token not in ignore_tokens:\n",
    "      tokens.append(token.replace('##', ''))\n",
    "  return '\\n'.join(tokens[:top_clean])"
   ],
   "id": "a085fc5be4282523",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T19:12:06.665470Z",
     "start_time": "2025-10-14T19:12:06.659470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def get_all_predictions(input_text, model_name, top_clean=5):\n",
    "    model_name = model_name.lower()\n",
    "\n",
    "    if model_name == \"bert\":\n",
    "        tok, mdl = load_model(\"bert\")\n",
    "        input_ids, mask_idx = encode_bert(tok, input_text, add_special_tokens=True)\n",
    "        outputs = mdl(input_ids)\n",
    "        logits = outputs.logits[0, mask_idx]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        topk = torch.topk(probs, k=top_clean)\n",
    "        pred_tokens = topk.indices.tolist()\n",
    "        return {\"bert\": decode_bert(tok, pred_tokens, top_clean)}\n",
    "\n",
    "    elif model_name == \"gpt\":\n",
    "        tok, mdl = load_model(\"gpt\")\n",
    "        enc = tok(input_text, return_tensors=\"pt\")\n",
    "        outputs = mdl(**enc)\n",
    "        logits = outputs.logits[0, -1, :]  # next-token distribution\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        topk = torch.topk(probs, k=top_clean)\n",
    "        ids = topk.indices.tolist()\n",
    "        # decode each candidate token by itself for cleaner strings\n",
    "        tokens = [tok.decode([i]).strip() for i in ids]\n",
    "        return {\"gpt\": \"\\n\".join(tokens)}\n",
    "\n",
    "    elif model_name == \"xlnet\":\n",
    "        tok, mdl = load_model(\"xlnet\")\n",
    "        # XLNet is permutation-based but AutoModelForCausalLM head gives next-token logits\n",
    "        enc = tok(input_text, return_tensors=\"pt\")\n",
    "        outputs = mdl(**enc)\n",
    "        logits = outputs.logits[0, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        topk = torch.topk(probs, k=top_clean)\n",
    "        ids = topk.indices.tolist()\n",
    "        tokens = [tok.decode([i]).strip() for i in ids]\n",
    "        return {\"xlnet\": \"\\n\".join(tokens)}\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model_name\")"
   ],
   "id": "672d4bc55a9e5c30",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T19:12:06.676911Z",
     "start_time": "2025-10-14T19:12:06.673377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_prediction_end_of_sentence(input_text, model_name):\n",
    "  try:\n",
    "    if model_name.lower() == \"bert\":\n",
    "      input_text += ' <mask>'\n",
    "      print(input_text)\n",
    "      res = get_all_predictions(input_text, model_name, top_clean=int(no_words_to_be_predicted))\n",
    "      return res\n",
    "    elif model_name.lower() == \"gpt\":\n",
    "      print(input_text)\n",
    "      res = get_all_predictions(input_text, model_name, top_clean=int(no_words_to_be_predicted))\n",
    "      return res\n",
    "    else:\n",
    "      print(input_text)\n",
    "      res = get_all_predictions(input_text, model_name, top_clean=int(no_words_to_be_predicted))\n",
    "      return res\n",
    "\n",
    "  except Exception as error:\n",
    "    pass"
   ],
   "id": "3074f1702d4f2a83",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T19:12:35.980503Z",
     "start_time": "2025-10-14T19:12:33.622636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "no_words_to_be_predicted, select_model, enter_input_text = set_model_config(no_words_to_be_predicted=2, select_model = \"gpt\", enter_input_text = \"To be or not\")\n",
    "\n",
    "bert_tokenizer, bert_model = load_model(select_model)\n",
    "res = get_prediction_end_of_sentence(enter_input_text, select_model)\n",
    "print(f\"The result is: {res[select_model].split(\"\\n\")}\")\n",
    "\n",
    "final_sentence = \" \".join([i for i in res[select_model].split(\"\\n\")])\n",
    "print(f\"The final structured sentence is: {enter_input_text + \" \" + final_sentence}\")\n"
   ],
   "id": "d5982bd5f59b72dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_words_to_be_predicted = 2\n",
      "select_model = gpt\n",
      "enter_input_text = To be or not\n",
      "Tokenizer: GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n",
      "), gpt_model: GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "To be or not\n",
      "Tokenizer: GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n",
      "), gpt_model: GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "The result is: ['to', 'be']\n",
      "The final structured sentence is: To be or not to be\n"
     ]
    }
   ],
   "execution_count": 47
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
