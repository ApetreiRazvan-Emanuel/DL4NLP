{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T05:41:42.996358Z",
     "start_time": "2025-10-15T05:41:40.433984Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install transformers",
   "id": "d1d4c1bf66eac194",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.57.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2025.10.5)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T05:42:48.689245Z",
     "start_time": "2025-10-15T05:42:43.692485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer"
   ],
   "id": "d20d23be6e3d16eb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apetr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T05:43:01.988478Z",
     "start_time": "2025-10-15T05:42:49.903465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "coqa = pd.read_json('http://downloads.cs.stanford.edu/nlp/data/coqa/coqa-train-v1.0.json')\n",
    "coqa.head()"
   ],
   "id": "db418946dc3e031",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   version                                               data\n",
       "0        1  {'source': 'wikipedia', 'id': '3zotghdk5ibi9ce...\n",
       "1        1  {'source': 'cnn', 'id': '3wj1oxy92agboo5nlq4r7...\n",
       "2        1  {'source': 'gutenberg', 'id': '3bdcf01ogxu7zdn...\n",
       "3        1  {'source': 'cnn', 'id': '3ewijtffvo7wwchw6rtya...\n",
       "4        1  {'source': 'gutenberg', 'id': '3urfvvm165iantk..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>{'source': 'wikipedia', 'id': '3zotghdk5ibi9ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>{'source': 'cnn', 'id': '3wj1oxy92agboo5nlq4r7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>{'source': 'gutenberg', 'id': '3bdcf01ogxu7zdn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>{'source': 'cnn', 'id': '3ewijtffvo7wwchw6rtya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>{'source': 'gutenberg', 'id': '3urfvvm165iantk...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T05:43:23.203660Z",
     "start_time": "2025-10-15T05:43:17.599718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "del coqa[\"version\"]\n",
    "cols = [\"text\",\"question\",\"answer\"]\n",
    "comp_list = []\n",
    "for index, row in coqa.iterrows():\n",
    "    for i in range(len(row[\"data\"][\"questions\"])):\n",
    "        temp_list = []\n",
    "        temp_list.append(row[\"data\"][\"story\"])\n",
    "        temp_list.append(row[\"data\"][\"questions\"][i][\"input_text\"])\n",
    "        temp_list.append(row[\"data\"][\"answers\"][i][\"input_text\"])\n",
    "        comp_list.append(temp_list)\n",
    "\n",
    "new_df = pd.DataFrame(comp_list, columns=cols)\n",
    "new_df.to_csv(\"CoQA_data.csv\", index=False)"
   ],
   "id": "c57fbba352b73a4a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T05:43:28.545461Z",
     "start_time": "2025-10-15T05:43:26.419880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(\"CoQA_data.csv\")\n",
    "data.head()"
   ],
   "id": "3dbdfb3ee346e7d2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                text  \\\n",
       "0  The Vatican Apostolic Library (), more commonl...   \n",
       "1  The Vatican Apostolic Library (), more commonl...   \n",
       "2  The Vatican Apostolic Library (), more commonl...   \n",
       "3  The Vatican Apostolic Library (), more commonl...   \n",
       "4  The Vatican Apostolic Library (), more commonl...   \n",
       "\n",
       "                            question                               answer  \n",
       "0  When was the Vat formally opened?  It was formally established in 1475  \n",
       "1           what is the library for?                             research  \n",
       "2                 for what subjects?                     history, and law  \n",
       "3                               and?     philosophy, science and theology  \n",
       "4          what was started in 2014?                           a  project  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>When was the Vat formally opened?</td>\n",
       "      <td>It was formally established in 1475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>what is the library for?</td>\n",
       "      <td>research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>for what subjects?</td>\n",
       "      <td>history, and law</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>and?</td>\n",
       "      <td>philosophy, science and theology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>what was started in 2014?</td>\n",
       "      <td>a  project</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T05:43:33.299466Z",
     "start_time": "2025-10-15T05:43:33.293720Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Number of question and answers: \", len(data))",
   "id": "9b6ed7b2fe743b8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of question and answers:  108647\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T05:48:20.985320Z",
     "start_time": "2025-10-15T05:44:48.580015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ],
   "id": "f46f1a31b8961f4d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apetr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\apetr\\.cache\\huggingface\\hub\\models--bert-large-uncased-whole-word-masking-finetuned-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T05:48:45.631712Z",
     "start_time": "2025-10-15T05:48:45.611144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "random_num = np.random.randint(0,len(data))\n",
    "question = data[\"question\"][random_num]\n",
    "text = data[\"text\"][random_num]\n",
    "\n",
    "input_ids = tokenizer.encode(question, text)\n",
    "print(\"The input has a total of {} tokens.\".format(len(input_ids)))\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "for token, id in zip(tokens, input_ids):\n",
    "    print('{:8}{:8,}'.format(token,id))"
   ],
   "id": "372b37b959f03f52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input has a total of 335 tokens.\n",
      "[CLS]        101\n",
      "can        2,064\n",
      "you        2,017\n",
      "find       2,424\n",
      "it         2,009\n",
      "in         1,999\n",
      "the        1,996\n",
      "dictionary   9,206\n",
      "?          1,029\n",
      "[SEP]        102\n",
      "a          1,037\n",
      "new        2,047\n",
      "word       2,773\n",
      "is         2,003\n",
      "becoming   3,352\n",
      "more       2,062\n",
      "and        1,998\n",
      "more       2,062\n",
      "popular    2,759\n",
      "on         2,006\n",
      "the        1,996\n",
      "internet   4,274\n",
      "in         1,999\n",
      "china      2,859\n",
      "-          1,011\n",
      "but        2,021\n",
      "no         2,053\n",
      "one        2,028\n",
      "knows      4,282\n",
      "quite      3,243\n",
      "what       2,054\n",
      "it         2,009\n",
      "means      2,965\n",
      ".          1,012\n",
      "the        1,996\n",
      "word       2,773\n",
      "\"          1,000\n",
      "du         4,241\n",
      "##ang      5,654\n",
      "\"          1,000\n",
      "is         2,003\n",
      "so         2,061\n",
      "new        2,047\n",
      "that       2,008\n",
      "you        2,017\n",
      "can        2,064\n",
      "'          1,005\n",
      "t          1,056\n",
      "even       2,130\n",
      "find       2,424\n",
      "it         2,009\n",
      "in         1,999\n",
      "the        1,996\n",
      "chinese    2,822\n",
      "dictionary   9,206\n",
      ".          1,012\n",
      "but        2,021\n",
      "it         2,009\n",
      "has        2,038\n",
      "already    2,525\n",
      "spread     3,659\n",
      "like       2,066\n",
      "fire       2,543\n",
      "on         2,006\n",
      "the        1,996\n",
      "chinese    2,822\n",
      "internet   4,274\n",
      ",          1,010\n",
      "appearing   6,037\n",
      "more       2,062\n",
      "than       2,084\n",
      "8          1,022\n",
      ",          1,010\n",
      "000        2,199\n",
      ",          1,010\n",
      "000        2,199\n",
      "times      2,335\n",
      "on         2,006\n",
      "wei       11,417\n",
      "##bo       5,092\n",
      ",          1,010\n",
      "where      2,073\n",
      "15         2,321\n",
      ",          1,010\n",
      "000        2,199\n",
      "users      5,198\n",
      "had        2,018\n",
      "more       2,062\n",
      "than       2,084\n",
      "312       21,036\n",
      ",          1,010\n",
      "000        2,199\n",
      "discussions  10,287\n",
      ".          1,012\n",
      "on         2,006\n",
      "bai       21,790\n",
      "##du       8,566\n",
      ",          1,010\n",
      "it         2,009\n",
      "has        2,038\n",
      "been       2,042\n",
      "looked     2,246\n",
      "up         2,039\n",
      "almost     2,471\n",
      "600        5,174\n",
      ",          1,010\n",
      "000        2,199\n",
      "times      2,335\n",
      ".          1,012\n",
      "but        2,021\n",
      "what       2,054\n",
      "does       2,515\n",
      "it         2,009\n",
      "mean       2,812\n",
      "?          1,029\n",
      "\"          1,000\n",
      "everyone   3,071\n",
      "'          1,005\n",
      "s          1,055\n",
      "du         4,241\n",
      "##ang      5,654\n",
      "-          1,011\n",
      "ing       13,749\n",
      "and        1,998\n",
      "i          1,045\n",
      "still      2,145\n",
      "don        2,123\n",
      "'          1,005\n",
      "t          1,056\n",
      "know       2,113\n",
      "what       2,054\n",
      "it         2,009\n",
      "means      2,965\n",
      "!            999\n",
      "looks      3,504\n",
      "like       2,066\n",
      "i          1,045\n",
      "'          1,005\n",
      "d          1,040\n",
      "better     2,488\n",
      "go         2,175\n",
      "back       2,067\n",
      "to         2,000\n",
      "school     2,082\n",
      "now        2,085\n",
      ",          1,010\n",
      "\"          1,000\n",
      "said       2,056\n",
      "wei       11,417\n",
      "##bo       5,092\n",
      "user       5,310\n",
      "fa         6,904\n",
      "##hmi     26,837\n",
      "##da       2,850\n",
      ".          1,012\n",
      "another    2,178\n",
      "user       5,310\n",
      "asked      2,356\n",
      ":          1,024\n",
      "\"          1,000\n",
      "have       2,031\n",
      "you        2,017\n",
      "du         4,241\n",
      "##ang      5,654\n",
      "-          1,011\n",
      "ed         3,968\n",
      "today      2,651\n",
      "?          1,029\n",
      "my         2,026\n",
      "mind       2,568\n",
      "is         2,003\n",
      "full       2,440\n",
      "of         1,997\n",
      "du         4,241\n",
      "##ang      5,654\n",
      "du         4,241\n",
      "##ang      5,654\n",
      "du         4,241\n",
      "##ang      5,654\n",
      ".          1,012\n",
      "\"          1,000\n",
      "\"          1,000\n",
      "to         2,000\n",
      "du         4,241\n",
      "##ang      5,654\n",
      "or         2,030\n",
      "not        2,025\n",
      "to         2,000\n",
      "du         4,241\n",
      "##ang      5,654\n",
      ",          1,010\n",
      "that       2,008\n",
      "is         2,003\n",
      "the        1,996\n",
      "question   3,160\n",
      ",          1,010\n",
      "\"          1,000\n",
      "wrote      2,626\n",
      "user       5,310\n",
      "beatrice  14,807\n",
      ".          1,012\n",
      "\"          1,000\n",
      "du         4,241\n",
      "##ang      5,654\n",
      "\"          1,000\n",
      "seems      3,849\n",
      "to         2,000\n",
      "be         2,022\n",
      "im        10,047\n",
      "##itating  16,518\n",
      "a          1,037\n",
      "sound      2,614\n",
      ".          1,012\n",
      "it         2,009\n",
      "all        2,035\n",
      "seems      3,849\n",
      "to         2,000\n",
      "have       2,031\n",
      "started    2,318\n",
      "with       2,007\n",
      "hong       4,291\n",
      "kong       4,290\n",
      "action     2,895\n",
      "star       2,732\n",
      "jackie     9,901\n",
      "chan       9,212\n",
      ",          1,010\n",
      "who        2,040\n",
      "in         1,999\n",
      "2004       2,432\n",
      "appeared   2,596\n",
      "in         1,999\n",
      "a          1,037\n",
      "sham      25,850\n",
      "##poo     24,667\n",
      "ad         4,748\n",
      "where      2,073\n",
      "he         2,002\n",
      "used       2,109\n",
      "the        1,996\n",
      "sound      2,614\n",
      "\"          1,000\n",
      "du         4,241\n",
      "##ang      5,654\n",
      "\"          1,000\n",
      "to         2,000\n",
      "describe   6,235\n",
      "his        2,010\n",
      "soft       3,730\n",
      "and        1,998\n",
      "black      2,304\n",
      "hair       2,606\n",
      ".          1,012\n",
      "the        1,996\n",
      "word       2,773\n",
      "came       2,234\n",
      "to         2,000\n",
      "people     2,111\n",
      "again      2,153\n",
      "recently   3,728\n",
      "after      2,044\n",
      "chan       9,212\n",
      "posted     6,866\n",
      "it         2,009\n",
      "on         2,006\n",
      "his        2,010\n",
      "wei       11,417\n",
      "##bo       5,092\n",
      "page       3,931\n",
      ".          1,012\n",
      "thousands   5,190\n",
      "of         1,997\n",
      "users      5,198\n",
      "then       2,059\n",
      "began      2,211\n",
      "to         2,000\n",
      "visit      3,942\n",
      "chan       9,212\n",
      "'          1,005\n",
      "s          1,055\n",
      "wei       11,417\n",
      "##bo       5,092\n",
      "page       3,931\n",
      "with       2,007\n",
      "comments   7,928\n",
      ".          1,012\n",
      "the        1,996\n",
      "word       2,773\n",
      "seems      3,849\n",
      "to         2,000\n",
      "have       2,031\n",
      "many       2,116\n",
      "different   2,367\n",
      "meanings  15,383\n",
      ",          1,010\n",
      "and        1,998\n",
      "there      2,045\n",
      "'          1,005\n",
      "s          1,055\n",
      "no         2,053\n",
      "perfect    3,819\n",
      "chinese    2,822\n",
      "meaning    3,574\n",
      "for        2,005\n",
      "it         2,009\n",
      ",          1,010\n",
      "but        2,021\n",
      "you        2,017\n",
      "could      2,071\n",
      "use        2,224\n",
      "it         2,009\n",
      "to         2,000\n",
      "give       2,507\n",
      "emphasis   7,902\n",
      "to         2,000\n",
      "the        1,996\n",
      "word       2,773\n",
      "that       2,008\n",
      "follows    4,076\n",
      "it         2,009\n",
      ".          1,012\n",
      "a          1,037\n",
      "kid        4,845\n",
      "might      2,453\n",
      "be         2,022\n",
      "\"          1,000\n",
      "du         4,241\n",
      "##ang      5,654\n",
      "cute      10,140\n",
      "\"          1,000\n",
      ",          1,010\n",
      "for        2,005\n",
      "example    2,742\n",
      ".          1,012\n",
      "[SEP]        102\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T05:49:11.346932Z",
     "start_time": "2025-10-15T05:49:09.919310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
    "print(\"SEP token index: \", sep_idx)\n",
    "\n",
    "num_seg_a = sep_idx+1\n",
    "print(\"Number of tokens in segment A: \", num_seg_a)\n",
    "\n",
    "num_seg_b = len(input_ids) - num_seg_a\n",
    "print(\"Number of tokens in segment B: \", num_seg_b)\n",
    "\n",
    "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "#token input_ids to represent the input and token segment_ids to differentiate our segments - question and text\n",
    "output = model(torch.tensor([input_ids]),  token_type_ids=torch.tensor([segment_ids]))\n",
    "\n",
    "answer_start = torch.argmax(output.start_logits)\n",
    "answer_end = torch.argmax(output.end_logits)\n",
    "\n",
    "if answer_end >= answer_start:\n",
    "    answer = \" \".join(tokens[answer_start:answer_end+1])\n",
    "else:\n",
    "    print(\"I am unable to find the answer to this question. Can you please ask another question?\")\n",
    "\n",
    "print(\"nQuestion:n{}\".format(question.capitalize()))\n",
    "print(\"nAnswer:n{}.\".format(answer.capitalize()))"
   ],
   "id": "4062dcbd6c53524e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEP token index:  9\n",
      "Number of tokens in segment A:  10\n",
      "Number of tokens in segment B:  325\n",
      "nQuestion:nCan you find it in the dictionary?\n",
      "nAnswer:nYou can ' t even find it in the chinese dictionary.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T05:50:05.417837Z",
     "start_time": "2025-10-15T05:50:05.406123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "answer = tokens[answer_start]\n",
    "\n",
    "for i in range(answer_start+1, answer_end+1):\n",
    "    if tokens[i][0:2] == \"##\":\n",
    "        answer += tokens[i][2:]\n",
    "    else:\n",
    "        answer += \" \" + tokens[i]"
   ],
   "id": "273c5f412be4732a",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T06:01:23.103846Z",
     "start_time": "2025-10-15T06:01:23.096071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def question_answer(question, text):\n",
    "\n",
    "    #tokenize question and text as a pair\n",
    "    input_ids = tokenizer.encode(question, text)\n",
    "\n",
    "    #string version of tokenized ids\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    #segment IDs\n",
    "    #first occurence of [SEP] token\n",
    "    sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    #number of tokens in segment A (question)\n",
    "    num_seg_a = sep_idx+1\n",
    "\n",
    "    #number of tokens in segment B (text)\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "    #list of 0s and 1s for segment embeddings\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    #model output using input_ids and segment_ids\n",
    "    output = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]))\n",
    "\n",
    "    #reconstructing the answer\n",
    "    answer_start = torch.argmax(output.start_logits)\n",
    "    answer_end = torch.argmax(output.end_logits)\n",
    "\n",
    "    if answer_end >= answer_start:\n",
    "        answer = tokens[answer_start]\n",
    "        for i in range(answer_start+1, answer_end+1):\n",
    "            if tokens[i][0:2] == \"##\":\n",
    "                answer += tokens[i][2:]\n",
    "            else:\n",
    "                answer += \" \" + tokens[i]\n",
    "\n",
    "    if answer.startswith(\"[CLS]\"):\n",
    "        answer = \"Unable to find the answer to your question.\"\n",
    "\n",
    "    print(\"nPredicted answer:n{}\".format(answer.capitalize()))\n",
    "    return answer.capitalize()"
   ],
   "id": "5506b95fc498181e",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T05:51:25.068572Z",
     "start_time": "2025-10-15T05:51:23.803742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"\"\"New York (CNN) -- More than 80 Michael Jackson collectibles -- including the late pop star's famous rhinestone-studded glove from a 1983 performance -- were auctioned off Saturday, reaping a total $2 million. Profits from the auction at the Hard Rock Cafe in New York's Times Square crushed pre-sale expectations of only $120,000 in sales. The highly prized memorabilia, which included items spanning the many stages of Jackson's career, came from more than 30 fans, associates and family members, who contacted Julien's Auctions to sell their gifts and mementos of the singer. Jackson's flashy glove was the big-ticket item of the night, fetching $420,000 from a buyer in Hong Kong, China. Jackson wore the glove at a 1983 performance during \"Motown 25,\" an NBC special where he debuted his revolutionary moonwalk. Fellow Motown star Walter \"Clyde\" Orange of the Commodores, who also performed in the special 26 years ago, said he asked for Jackson's autograph at the time, but Jackson gave him the glove instead. \"The legacy that [Jackson] left behind is bigger than life for me,\" Orange said. \"I hope that through that glove people can see what he was trying to say in his music and what he said in his music.\" Orange said he plans to give a portion of the proceeds to charity. Hoffman Ma, who bought the glove on behalf of Ponte 16 Resort in Macau, paid a 25 percent buyer's premium, which was tacked onto all final sales over $50,000. Winners of items less than $50,000 paid a 20 percent premium.\"\"\"\n",
    "\n",
    "question = \"Where was the Auction held?\"\n",
    "\n",
    "question_answer(question, text)\n",
    "\n",
    "print(\"Original answer:n\", data.loc[data[\"question\"] == question][\"answer\"].values[0])\n"
   ],
   "id": "56f9a26afc4a0318",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nPredicted answer:nHard rock cafe in new york ' s times square\n",
      "Original answer:n Hard Rock Cafe\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T06:00:04.425255Z",
     "start_time": "2025-10-15T05:59:59.828748Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install deep-translator",
   "id": "bfc077e0d6cd123a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deep-translator\n",
      "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from deep-translator) (4.14.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from deep-translator) (2.32.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\apetr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.10.5)\n",
      "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
      "Installing collected packages: deep-translator\n",
      "Successfully installed deep-translator-1.11.4\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T06:00:21.275292Z",
     "start_time": "2025-10-15T06:00:21.270535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def translate_to_romanian(text):\n",
    "    from deep_translator import GoogleTranslator\n",
    "\n",
    "    try:\n",
    "        translated = GoogleTranslator(source='auto', target='ro').translate(text)\n",
    "        return translated\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return text"
   ],
   "id": "54ea77ae97c6d84f",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T06:35:55.684772Z",
     "start_time": "2025-10-15T06:35:22.650018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = input(\"Please enter your text:\")\n",
    "question = input(\"Please enter your question:\")\n",
    "\n",
    "while True:\n",
    "    answer = question_answer(question, text)\n",
    "    print(f\"Answer in romanian:\\n{translate_to_romanian(answer)}\")\n",
    "    flag = True\n",
    "    flag_N = False\n",
    "\n",
    "    while flag:\n",
    "        response = input(\"Do you want to ask another question based on this text (Y/N)? \")\n",
    "        if response[0] == \"Y\":\n",
    "            question = input(\"Please enter your question:\")\n",
    "            flag = False\n",
    "        elif response[0] == \"N\":\n",
    "            print(\"Bye!\")\n",
    "            flag = False\n",
    "            flag_N = True\n",
    "\n",
    "    if flag_N == True:\n",
    "        break"
   ],
   "id": "f67f93d883b31629",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nPredicted answer:nVideo games\n",
      "Answer in romanian:\n",
      "Jocuri video\n",
      "nPredicted answer:nProgrammer\n",
      "Answer in romanian:\n",
      "Programator\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[33]\u001B[39m\u001B[32m, line 11\u001B[39m\n\u001B[32m      8\u001B[39m flag_N = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m flag:\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m     response = \u001B[38;5;28;43minput\u001B[39;49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mDo you want to ask another question based on this text (Y/N)? \u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     12\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m response[\u001B[32m0\u001B[39m] == \u001B[33m\"\u001B[39m\u001B[33mY\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m     13\u001B[39m         question = \u001B[38;5;28minput\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mPlease enter your question:\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ipykernel\\kernelbase.py:1473\u001B[39m, in \u001B[36mKernel.raw_input\u001B[39m\u001B[34m(self, prompt)\u001B[39m\n\u001B[32m   1471\u001B[39m     msg = \u001B[33m\"\u001B[39m\u001B[33mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1472\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(msg)\n\u001B[32m-> \u001B[39m\u001B[32m1473\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_input_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1474\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1475\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_shell_parent_ident\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1476\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_parent\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mshell\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1477\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpassword\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1478\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ipykernel\\kernelbase.py:1518\u001B[39m, in \u001B[36mKernel._input_request\u001B[39m\u001B[34m(self, prompt, ident, parent, password)\u001B[39m\n\u001B[32m   1515\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[32m   1516\u001B[39m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[32m   1517\u001B[39m     msg = \u001B[33m\"\u001B[39m\u001B[33mInterrupted by user\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1518\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1519\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   1520\u001B[39m     \u001B[38;5;28mself\u001B[39m.log.warning(\u001B[33m\"\u001B[39m\u001B[33mInvalid Message:\u001B[39m\u001B[33m\"\u001B[39m, exc_info=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: Interrupted by user"
     ]
    }
   ],
   "execution_count": 33
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
